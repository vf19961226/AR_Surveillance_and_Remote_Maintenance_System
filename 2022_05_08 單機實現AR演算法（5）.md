# 2022/05/08 單機實現AR演算法（5）
###### tags: `論文實做紀錄` `AR演算法`
## 實作環境
參考2022/04/26 單機實現AR演算法（1）的[實作環境](https://hackmd.io/wpSYJl26Rw-ScjVlzjYwYw?view#%E5%AF%A6%E4%BD%9C%E7%92%B0%E5%A2%83)。

## 實作紀錄
### OpenGL影像輸出
因論文架構中需將AR演算法進行拆分，將其部署於不同設備上進行運算，並利用網路傳輸資料，故需將渲染完成的影像進行輸出，使AR設備可透過網路獲取AR渲染影像。程式步驟如下所述。

1. **獲取影像**
使用`glReadPixels()`函式獲取片段緩衝器（Frame Buffer）影像，並將其回傳值使用`np.frombuffer()`轉換為一個浮點數陣列，最終將根據影像高度、寬度、通道數重新塑形為三維矩陣。程式如下所示。程式中的相關參數意義如下表所示。

```python=
img = glReadPixels(x, y, width, height, format, type)
img = np.frombuffer(img, np.float32)
img.shape = height, width, channel
```

|參數名稱|參數意義|
|:---:|:---
|img|OpenGL影像緩衝器輸出之影像數據
|x|指定從影像緩衝器讀取的第一個像素的X座標，通常使用原點
|y|指定從影像緩衝器讀取的第一個像素的Y座標，通常使用原點
|width|指定輸出影像的寬
|height|指定輸出影像的高
|format|指定輸出影像數據格式，影像數據格式可參考[glReadPixels函式說明頁面](http://pyopengl.sourceforge.net/documentation/manual-3.0/glReadPixels.html)。
|type|指定輸出影像數據的數據類型，數據類型可參考[glReadPixels函式說明頁面](http://pyopengl.sourceforge.net/documentation/manual-3.0/glReadPixels.html)。
|channel|影像通道數，如RGB影像為3通道

2. **儲存影像**
使用`cv2.imwrite()`函式儲存影像，在影像儲存前須將其由RGB轉換為OpenCV所使用的BGR格式，並將其值放大255倍，以符合OpenCV要求。程式如下所示。

```python=
img = img[::-1, :] #RGB to BGR
cv2.imwrite(save_path, img * 255) #save image
```

### 簡易AR測試
以[2022/05/05 單機實現AR演算法（4）的OpenGL相機設定](https://hackmd.io/RUoE5QyxQ5ywffnHCvqi-w?view#OpenGL%E7%9B%B8%E6%A9%9F%E8%A8%AD%E5%AE%9A)為基礎，將相機影像做為背景，使用OpenGL影像貼圖的方式，將其顯示於虛擬物件後方，並占滿視窗，如下圖所示。程式步驟如下所述。

![簡易AR測試](https://i.imgur.com/BCK6oif.png)

1. **取得相機影像**
使用OpenCV獲取相機影像，詳細可參考[2022/04/26 單機實現AR演算法（1）的ArUco標籤辨識](https://hackmd.io/wpSYJl26Rw-ScjVlzjYwYw?view#ArUco%E6%A8%99%E7%B1%A4%E8%BE%A8%E8%AD%98)，其中第二步與第三部使用OpenCV獲取相機影像，並進行影像處理。

2. **設定投影矩陣**
因背景貼圖需顯示於背景中，且填滿整個視窗，故須先設定投影矩陣，且因其不會跟著相機移動而對背景貼圖位置有所改變，故不需設置視圖矩陣。本次實作將使用`gluPerspective()`函式進行投影矩陣計算，下圖表示了`gluPerspective()`函式各參數於OpenGL空間中的意義。各參數計算方式如下所述。程式如下所示。

![投影矩陣示意圖](https://i.imgur.com/8lSqJ8y.png)

* $fovy$：代表相機視角大小，單位為角度。計算方式如下，其中$height$為視窗高度，$\beta$為焦距在Y軸上的距離。
$$fovy = 2 * tan^{-1}(\frac {heidht} {2\beta})$$

* $aspect$：定義物體在視窗中的比例。計算方式如下，其中$\beta$為焦距在Y軸上的距離、$\alpha$為焦距在X軸上的距離、$width$為視窗寬度、$height$為視窗高度。
$$ascept = \frac {\beta} {\alpha} \frac {width} {height}$$

* $near$：定義距離相機最近的物體截面距離。<font color=#FF0000>**若物體移動到小於這個距離將不會被繪製。**</font>

* $far$：定義距離相機最遠的物體截面距離。<font color=#FF0000>**若物體移動到大於這個距離將不會被繪製。**</font>

```python=
glMatrixMode(GL_PROJECTION)
glLoadIdentity()
gluPerspective(fovy, aspect, near, far)
```

3. **影像格式轉換**
因OpenCV與OpenGL坐標系的不同（如下圖），故須先使用`cv2.flip()`函式繞X軸進行翻轉，接著使用`PIL.Image.fromarry()`函式將影像轉換為Pillow格式，最終將其使用raw編碼器轉換為bytes格式，後方參數為編碼器相關參數，詳細可參考[這裡](https://stackoverflow.com/questions/64074990/using-pillow-image-tobytes-to-flip-the-image-and-swap-the-color-channels)和[這裡](https://www.geeksforgeeks.org/python-pil-tobytes-method/)。

![cv2gl coordinates](https://i.imgur.com/OTCUseo.png)


```python=
bg_image = cv2.flip(image, 0)
bg_image = PIL.Image.fromarray(bg_image)
bg_image = bg_image.tobytes("raw", "BGRX", 0, -1)
```

4. **創建背景貼圖**
首先使用`glGenTextures()`函式創建貼圖編號，接著使用`glBindTexture()`指定貼圖格式，並使用`glTexParameter()`函式設定貼圖映射參數，最終使用`glTexImage2D()`函式將背景圖片作為貼圖輸入到OpenGL中。接著使用`glBegin()`與`glEnd()`函式進行繪圖，但該方法為OpenGL1.1之舊版方法，未來將改用Buffer與Shader方式進行繪製，故在此將布詳細介紹。在完成貼圖繪製之後再次使用`glBindTexture()`取消指定貼圖格式（指定編號為0）。程式如下所示。

```python=
textures = glGenTextures(n)
glBindTexture(target, textures)
glTexParameterf(target, pname, param)
glTexImage2D(target, Level, internalformat, width, height, border, format, type, data)

##########舊方法############
glTranslatef(0.0,0.0,-10.0)
glBegin(GL_QUADS)
glTexCoord2f(0.0, 1.0); glVertex3f(-4.0, -3.0, 0.0)
glTexCoord2f(1.0, 1.0); glVertex3f( 4.0, -3.0, 0.0)
glTexCoord2f(1.0, 0.0); glVertex3f( 4.0,  3.0, 0.0)
glTexCoord2f(0.0, 0.0); glVertex3f(-4.0,  3.0, 0.0)
glEnd()
###########################

glBindTexture(target, 0)
```

|參數名稱|參數意義|
|:---:|:---
|textures|存放貼圖編號的物件
|n|貼圖的數量
|target|貼圖的形式
|pname|貼圖參數名稱，可參考《OpenGL 3D繪圖互動程式設計》p11-25 11-3-2節
|param|貼圖參數的值，可參考《OpenGL 3D繪圖互動程式設計》p11-29 11-3-3節
|Level|貼圖解析度等級，0為最大，數值越大解析度越低
|internalformat|儲存貼圖的格式（RGB或RGBA等）
|width|貼圖的寬度
|height|貼圖的高度
|border|必須為0（OpenGL3.0後不再支援）
|format|輸入貼圖的格式，如通道順序為RGBA或BGR等
|type|貼圖圖片中每個像素的資料型態，如GL_BYTE、GL_FLOAT等
|data|貼圖圖片資料

## 程式碼整理
### OpenGL影像輸出
```python=
import cv2
import numpy as np

from OpenGL.GLUT import *
from OpenGL.GLU import *
from OpenGL.GL import *

def save_img():
    save_path = 'OpenGL_img.png'

    img_data = glReadPixels(0, 0, 640, 480, GL_BGRA, GL_FLOAT)
    img_data = np.frombuffer(img_data, np.float32)
    img_matrix = img_data
    img_data.shape = 480, 640, 4
    img_data = img_data[::-1, :]

    cv2.imwrite(save_path, img_data * 255)
    
def display():   
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
    glClearColor(0, 0, 0, 0)
    
    #draw teapot
    glMatrixMode(GL_MODELVIEW)
    glLoadIdentity()
    gluLookAt(0.0, 2.0, 5.0,
              0.0, 0.0, 0.0,
              0.0, 1.0, 0.0)
    
    glColor3b(125, 0, 0)
    glutSolidTeapot(0.5)
    
    save_img()
    
    glutSwapBuffers()
    
glutInit()
glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH)
glutInitWindowPosition(100, 100)
glutInitWindowSize(600, 600)
glutCreateWindow(b"OpenGL")
glClearColor(0, 0, 0, 0)
glutDisplayFunc(display)
glutMainLoop()
```

### 簡易AR測試
```python=
from OpenGL.GL import *
from OpenGL.GLUT import *
from OpenGL.GLU import *

import cv2
import numpy as np
from PIL import Image

class Filter:
    def __init__(self):
        self.pre_trans_x = None
        self.pre_trans_y = None
        self.pre_trans_z = None
        
    def update(self, tvecs) -> bool:
        trans_x, trans_y, trans_z = tvecs[0][0][0], tvecs[0][0][1], tvecs[0][0][2]
        is_mark_move = False
        if self.pre_trans_x is not None:
            if abs(self.pre_trans_x - trans_x) > 0.001 or abs(self.pre_trans_y - trans_y) > 0.002 or abs(self.pre_trans_z - trans_z) > 0.015:
                dis_x = abs(self.pre_trans_x - trans_x)
                dis_y = abs(self.pre_trans_y - trans_y)
                dis_z = abs(self.pre_trans_z - trans_z)
                # if dis_x > 0.001:
                #     print('dis_x', dis_x)
                # if dis_y > 0.001:
                #     print("dis_y", dis_y)
                # if dis_z > 0.001:
                #     print("dis_z", dis_z)
                
                is_mark_move = True
        self.pre_trans_x, self.pre_trans_y, self.pre_trans_z = trans_x, trans_y, trans_z
        return is_mark_move
        

def intrinsic2Project(MTX, width, height, near_plane=0.01, far_plane=100.0):
    """[Get ]
    Arguments:
        MTX {[np.array]} -- [The camera instrinsic matrix that you get from calibrating your chessboard]
        width {[float]} -- [width of viewport]]
        height {[float]} -- [height of viewport]
    Keyword Arguments:
        near_plane {float} -- [near_plane] (default: {0.01})
        far_plane {float} -- [far plane] (default: {100.0})
    Returns:
        [np.array] -- [1 dim array of project matrix]
    """
    P = np.zeros(shape=(4, 4), dtype=np.float32)

    fx, fy = MTX[0, 0], MTX[1, 1]
    cx, cy = MTX[0, 2], MTX[1, 2]

    P[0, 0] = 2 * fx / width
    P[1, 1] = 2 * fy / height
    P[2, 0] = 1 - 2 * cx / width
    P[2, 1] = 2 * cy / height - 1
    P[2, 2] = -(far_plane + near_plane) / (far_plane - near_plane)
    P[2, 3] = -1.0
    P[3, 2] = - (2 * far_plane * near_plane) / (far_plane - near_plane)

    return P.flatten()
    
def extrinsic2ModelView(RVEC, TVEC, R_vector = True):
    """[Get modelview matrix from RVEC and TVEC]
    Arguments:
        RVEC {[vector]} -- [Rotation vector]
        TVEC {[vector]} -- [Translation vector]
    """

    R, _ = cv2.Rodrigues(RVEC)

    Rx = np.array([
        [1, 0, 0],
        [0, -1, 0],
        [0, 0, -1]
    ])

    TVEC = TVEC.flatten().reshape((3, 1))

    #transform_matrix = Rx @ np.hstack((R, TVEC))
    transform_matrix = np.dot(Rx, np.hstack((R, TVEC)))
    M = np.eye(4)
    
    M[:3, :] = transform_matrix
    return M.T.flatten()

webcam = cv2.VideoCapture(0)
image_w, image_h = map(int, (webcam.get(3), webcam.get(4)))
parameter = np.load('test_Camera_parameter.npz')
cameraMatrix = parameter['mtx']
distCoeffs = parameter['dist']
model_scale = 0.03

projectMatrix = intrinsic2Project(cameraMatrix, image_w, image_h, 0.01, 100.0)

translate_x, translate_y, translate_z = 0, 0, 0
pre_extrinsicMatrix = None

filter = Filter()

def draw_background(image):
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)

    glMatrixMode(GL_PROJECTION)
    glLoadIdentity()
    gluPerspective(33.7, 1.3, 0.1, 100.0)
    glMatrixMode(GL_MODELVIEW)
    glLoadIdentity()

    # Convert image to OpenGL texture format
    bg_image = cv2.flip(image, 0)
    bg_image = Image.fromarray(bg_image)     
    ix = bg_image.size[0]
    iy = bg_image.size[1]
    bg_image = bg_image.tobytes("raw", "BGRX", 0, -1)

    # Create background texture
    texid = glGenTextures(1)
    glBindTexture(GL_TEXTURE_2D, texid)
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)
    glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST)
    glTexImage2D(GL_TEXTURE_2D, 0, 3, ix, iy, 0, GL_RGBA, GL_UNSIGNED_BYTE, bg_image)

    glTranslatef(0.0,0.0,-10.0)
    glBegin(GL_QUADS)
    glTexCoord2f(0.0, 1.0); glVertex3f(-4.0, -3.0, 0.0)
    glTexCoord2f(1.0, 1.0); glVertex3f( 4.0, -3.0, 0.0)
    glTexCoord2f(1.0, 0.0); glVertex3f( 4.0,  3.0, 0.0)
    glTexCoord2f(0.0, 0.0); glVertex3f(-4.0,  3.0, 0.0)
    glEnd()

    glBindTexture(GL_TEXTURE_2D, 0)
    
def draw_objects(image, mark_size = 0.05):
    global cameraMatrix, distCoeffs, pre_extrinsicMatrix, translate_x, translate_y, translate_z, filter, model_scale

    # aruco data
    aruco_dict = cv2.aruco.Dictionary_get(10)        
    parameters =  cv2.aruco.DetectorParameters_create()
    parameters.adaptiveThreshConstant = True

    height, width, channels = image.shape
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict, parameters = parameters)

    rvecs, tvecs, model_matrix = None, None, None

    if ids is not None and corners is not None:
        rvecs, tvecs, _= cv2.aruco.estimatePoseSingleMarkers(corners, mark_size , cameraMatrix, distCoeffs)
        new_rvecs = rvecs[0,:,:]
        new_tvecs = tvecs[0,:,:]

    projectMatrix = intrinsic2Project(cameraMatrix, width, height, 0.01, 100.0)
    glMatrixMode(GL_PROJECTION)
    glLoadIdentity()
    glMultMatrixf(projectMatrix)

    glMatrixMode(GL_MODELVIEW)
    glLoadIdentity()

    if tvecs is not None:
        if filter.update(tvecs): # the mark is moving
            model_matrix = extrinsic2ModelView(rvecs, tvecs)
        else:
            model_matrix = pre_extrinsicMatrix
    else:
        model_matrix =  pre_extrinsicMatrix


    if model_matrix is not None:     
        pre_extrinsicMatrix = model_matrix
        glLoadMatrixf(model_matrix)
        glScaled(model_scale, model_scale, model_scale)
        glTranslatef(translate_x, translate_y, translate_y)
        glutWireTeapot(1)

    cv2.imshow("Frame",image)
    cv2.waitKey(20)

def draw_scene():
    global webcam

    _, image = webcam.read()# get image from webcam camera.

    draw_background(image)  # draw background
    draw_objects(image, mark_size = 0.06) # draw the 3D objects.
    glutSwapBuffers()
    
def initOpengl(width, height, pos_x = 500, pos_y = 500, window_name = b'AR test'):
    glutInit()
    glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH)
    glutInitWindowSize(width, height)
    glutInitWindowPosition(pos_x, pos_y)

    window_id = glutCreateWindow(window_name)
    glutDisplayFunc(draw_scene)
    glutIdleFunc(draw_scene)

    glClearColor(0.0, 0.0, 0.0, 0.0)
    glClearDepth(1.0)
    glShadeModel(GL_SMOOTH)
    glDepthFunc(GL_LESS)
    glEnable(GL_DEPTH_TEST)

    # # Assign texture
    glEnable(GL_TEXTURE_2D)

    # Set ambient lighting
    glLightfv(GL_LIGHT0, GL_DIFFUSE, (0.5,0.5,0.5,1))
    
    glutMainLoop()
    
def main():
    global image_w, image_h

    initOpengl(image_w, image_h)
    
if __name__ == "__main__":
    main()
```

## Reference
### OpenGL影像輸出

[Python OpenGL.GL.glReadPixels() Examples](https://www.programcreek.com/python/example/58483/OpenGL.GL.glReadPixels)

### 簡易AR測試
[BryceQing/OPENCV_AR](https://github.com/BryceQing/OPENCV_AR/tree/7c81838f1c080fc6f9027ab46047c619da044c44)
一個簡單AR測試，使用Python進行實作，但是沒有建置Buffer與Shader，將造成選染頂點會一直輸入，最終佔滿GPU記憶體。

[Calibrated Cameras and gluPerspective](https://ksimek.github.io/2013/06/18/calibrated-cameras-and-gluperspective/)
gluPerspective計算方法。

[OpenGL（六） gluLookAt和gluPerspective函数解析](https://blog.csdn.net/dcrmg/article/details/53106457)
gluPerspective參數意義，以及與gluLookAt的比較。

