# 2022/05/05 單機實現AR演算法（4）
###### tags: `論文實做紀錄` `AR演算法`
## 實作環境
參考2022/04/26 單機實現AR演算法（1）的[實作環境](https://hackmd.io/wpSYJl26Rw-ScjVlzjYwYw?view#%E5%AF%A6%E4%BD%9C%E7%92%B0%E5%A2%83)。

## 實作紀錄
### OpenGL相機設定
將OpenCV獲得的相機參數輸入至OpenGL的相機中進行設定，使虛擬物件能夠跟隨現實中相機位置進行平移、旋轉、縮放等變換，結果將虛擬物件呈現於真實場景的ArUco標籤位置上，但目前尚未將真實影像匯入OpenGL進行渲染，故目前沒有使用背景，如下圖所示。程式步驟如下所述。

![OpenGL相機設定結果](https://i.imgur.com/dx3Et9p.jpg)

1. **獲取相機參數**
使用Numpy中的load函示匯入事先取得的相機矩陣、畸變係數等相機參數，並根據實際影像中的ArUco標籤位置進行相機姿態估計，以獲取平儀與旋轉矩陣，詳細實作步驟可參考[2022/04/27 單機實現AR演算法（2）](https://hackmd.io/yT3kcorQQ-aOUcm4TvPo6g?view#20220427-%E5%96%AE%E6%A9%9F%E5%AF%A6%E7%8F%BEAR%E6%BC%94%E7%AE%97%E6%B3%95%EF%BC%882%EF%BC%89)。

2. **矩陣轉換**
因OpenCV與OpenGL坐標系不相同（[參考](https://medium.com/comerge/what-are-the-coordinates-225f1ec0dd78)），且我們一般使用的矩陣以列為主（Row-Major），但在渲染語言中矩陣以行為主（Column-Major），所以要先將矩陣資料格式依據OpenGL格式進行轉換，並生成塑模座標矩陣（Model View Matrix）與投影矩陣（Projection Matrix），這些矩陣將用於OpenGL中的虛擬物件座標轉換，轉換流程如下圖所示，詳細內容可參考[這裡](https://amytabb.com/tips/tutorials/2019/06/28/OpenCV-to-OpenGL-tutorial-essentials/)。轉換公式如下所示。

![座標轉換](https://i.imgur.com/xKuA4MT.png)

$$ x_{GL} = \underbrace {{\bf {NDC}} \cdot {\bf {K_{GL}}}}_{\bf {M_{projection}}} \cdot \underbrace {\begin{bmatrix}  & {\bf {R}} &  & {\bf {t}} \\ 0 & 0 & 0 & 1 \\ \end{bmatrix}}_{\bf {M_{model \; view}}} \cdot {\bf {X}} $$  

* $x_{GL}$（影像座標）：物件在螢幕中顯示的座標
* ${\bf {NDC}}$（標準化設備座標，Normalized Device Coordinate，NDC）：4×4的方陣，由內部相機校正矩陣數值生成
* ${\bf {K_{GL}}}$（）：4×4的方陣，由內部相機校正矩陣數值生成
* ${\bf {R}}$（旋轉矩陣）：一個3×3的上三角矩陣，由相機姿態估計獲得
* ${\bf {t}}$（平移矩陣）：一個3×1的矩陣，由相機姿態估計獲得
* ${\bf {X}}$（世界座標）：物件在真實世界中的座標（電腦世界中）
* ${\bf {M_{projection}}}$：OpenGL座標轉換中的投影矩陣（Projection Matrix）
* ${\bf {M_{model \; view}}}$：OpenGL座標轉換中的塑模座標矩陣（Model View Matrix）

塑模座標矩陣（Model View Matrix）轉換程式實現如下所示。

```python=
def extrinsic2ModelView(RVEC, TVEC, R_vector = True):
    """[Get modelview matrix from RVEC and TVEC]
    Arguments:
        RVEC {[vector]} -- [Rotation vector]
        TVEC {[vector]} -- [Translation vector]
    """

    R, _ = cv2.Rodrigues(RVEC)

    Rx = np.array([
        [1, 0, 0],
        [0, -1, 0],
        [0, 0, -1]
    ])

    TVEC = TVEC.flatten().reshape((3, 1))

    #transform_matrix = Rx @ np.hstack((R, TVEC))
    transform_matrix = np.dot(Rx, np.hstack((R, TVEC)))
    M = np.eye(4)
    
    M[:3, :] = transform_matrix
    return M.T.flatten()
```

投影矩陣（Projection Matrix）轉換程式實現如下所示。
```python=
def intrinsic2Project(MTX, width, height, near_plane=0.01, far_plane=100.0):
    """[Get ]
    Arguments:
        MTX {[np.array]} -- [The camera instrinsic matrix that you get from calibrating your chessboard]
        width {[float]} -- [width of viewport]]
        height {[float]} -- [height of viewport]
    Keyword Arguments:
        near_plane {float} -- [near_plane] (default: {0.01})
        far_plane {float} -- [far plane] (default: {100.0})
    Returns:
        [np.array] -- [1 dim array of project matrix]
    """
    P = np.zeros(shape=(4, 4), dtype=np.float32)

    fx, fy = MTX[0, 0], MTX[1, 1]
    cx, cy = MTX[0, 2], MTX[1, 2]

    P[0, 0] = 2 * fx / width
    P[1, 1] = 2 * fy / height
    P[2, 0] = 1 - 2 * cx / width
    P[2, 1] = 2 * cy / height - 1
    P[2, 2] = -(far_plane + near_plane) / (far_plane - near_plane)
    P[2, 3] = -1.0
    P[3, 2] = - (2 * far_plane * near_plane) / (far_plane - near_plane)

    return P.flatten()
```

上述程式中的參數意義如下所示。

|參數名稱|參數意義|
|:---:|:---
|RVEC|相機旋轉矩陣，由相機姿態估計獲得
|TVEC|相機平移矩陣，由相機姿態估計獲得
|MTX|相機矩陣，由相機校正獲得
|width|以像素為單位，視窗的寬度
|height|以像素為單位，視窗的高度
|near_plane|近平面距離
|far_plane|遠平面距離

3. **設定OpenGL運算矩陣**
將轉換後的矩陣使用`glLoadMatrixf()`函式輸入至OpenGL中，將原本的矩陣替換掉後進行繪圖作業，繪圖詳細說明可參考[2022/04/29 單機實現AR演算法（3）](https://hackmd.io/cI9JoMX9S4epY58TWmEB-w?view#20220429-%E5%96%AE%E6%A9%9F%E5%AF%A6%E7%8F%BEAR%E6%BC%94%E7%AE%97%E6%B3%95%EF%BC%883%EF%BC%89)。程式如下所示。程式中的相關參數意義如下表所示。

```python=
def display():
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
    glClearColor(255, 255, 255, 0) #背景設定為白色
    
    glMatrixMode(GL_PROJECTION) #指定目前運算矩陣為投影矩陣
    glLoadIdentity()
    glMultMatrixf(projection) #將目前矩陣替換為前述根據相機校正結果轉換之投影矩陣

    glMatrixMode(GL_MODELVIEW) #指定目前運算矩陣為視圖矩陣
    glLoadIdentity()
    glLoadMatrixf(modelView) #將目前矩陣替換為前述根據相機校正結果轉換之視圖矩陣
    
    glColor3b(0, 125, 0)
    glutWireTeapot(0.5)
    
    glutSwapBuffers()
```

|參數名稱|參數意義|
|:---:|:---
|projection|投影矩陣，相機內部參數經由前述矩陣轉換之結果
|modelView|視圖矩陣，相機外部參數經由前述矩陣轉換之結果

<font color=#FF0000>**注意**</font>：投影矩陣或視圖矩陣輸入時，若為列為主（Row-Major）矩陣，需先轉置在輸入，且其輸入後會將矩陣以列為主拉平（flatten），並以行為主（Column-Major）矩陣進行重組，若沒有轉置直接輸入會造成影像輸出結果錯誤。

### 影像渲染器（Shader）與緩衝器（Buffer）建置範例練習
渲染器定義了資料該如何進行渲染，而緩衝器是CPU與GPU之間的溝通媒介，緩衝器存在於GPU記憶體中，根據緩衝器種類的不同，可以讀取或存入不同的資料。本日實作將渲染頂點位置以及頂點色彩傳送至緩衝器後經由渲染器進行渲染，結果將呈現一個彩色三角形，如下圖所示。程式實作步驟如下所示。

![Shader和Buffer實作成果](https://i.imgur.com/OH8k68l.jpg)

1. **建立渲染器**
首先使用`glCreateShader()`建立渲染器，並使用`glShaderSource()`將渲染程式碼輸入至渲染器中，最終使用`glCompileShader()`進行渲染程式編譯。在渲染器的使用上需與程式物件進行綁定，可使用`glCreateProgram()`函式建立一個空的程式物件，程式物件可將多個渲染器使用`glAttachShader()`附加在裡面組成渲染路徑，最終使用`glLinkProgram()`連結渲染器與程式物件，在最終使用上只需呼叫`glUseProgram()`就可以進行一連串自定義的渲染程序。程式如下所示。程式中的相關參數意義如下表所示。

```python=
shader = glCreaterShader(shaderType)
glShaderSource(shader, count, shader_code, length)
glCompileShader(shader)

program = glCreateProgram()
glAttachShader(program, shader)
glLinkProgram(program)
glUseProgram(program)
```

|參數名稱|參數意義|
|:---:|:---
|shader|渲染器物件名稱
|shaderType|產生的渲染器種類，種類可參考《OpenGL 3D繪圖互動程式設計》p4-24
|count|程式碼是由幾段字串所組成
|shader_code|渲染器的渲染程式，型別為字串（String）
|length|每個對應程式碼的長度，基本上會與shader_code的長度一致，如果輸入0或None會取整個程式碼字串去做組合，其他則根據數值從字串中取出一段做組合
|program|程式物件名稱

2. **建立緩衝器**
首先使用`glGenBuffers()`函式創建緩衝器，並使用`glBindBuffer()`函式設定緩衝器型態，接著使用`glBufferData()`函式設定緩衝器記憶體、存入的資料、使用方式等參數。在使用如頂點緩衝物件（Vertex Buffer Object，VBO），需使用`glGenVertexArrays()`函式創建頂點陣列物件（Vertex Array Object，VAO），VAO主要用以儲存VBO中頂點資料相關分配規則，並以`glBindVertexArray()`函式啟動VAO，需注意的是OpenGL會使用最後一個啟動的VAO，故VAO需使用此函式進行切換，最終使用`glVertexAttribPointer()`函式進行資料分配設定，設定完成後就可以使用`glEnableVertexAttribArray()`啟用VAO，並進行渲染作業。程式如下所示。程式中的相關參數意義如下表所示。

```python=
buffer = glGenBuffers(n)    
glBindBuffer(target, buffer)
glBufferData(target, data_size, data, usage)

vao = glGenVertexArrays(m)
glBindVertexArray(vao)
glVertexAttribPointer(index, per_size, elem_type, normalized, stride, pointer)
glEnableVertexAttribArray(index)
```

|  參數名稱|參數意義|
|:---:|:---
|buffer|緩衝器物件名稱
|n|產生的緩衝物件數量
|target|緩衝器的綁定目標，可決定緩衝器種類，綁定目標可參考《OpenGL 3D繪圖互動程式設計》p5-7 表05-002
|data_size|給定緩衝物件的資料儲存空間大小
|data|要存入的資料，如果沒有要傳入的資料，可填None
|usage|設定存入資料的使用方式，使用方式設置可參考《OpenGL 3D繪圖互動程式設計》p5-8 表05-003 與 表05-004
|vao|頂點陣列物件名稱
|m|產生的頂點陣列物件數量
|index|頂點屬性的索引值，需對應渲染器中宣告的頂點屬性索引
|per_size|每個屬性的資料所占用的大小是多少（有多少個元素）
|elem_type|填入屬性的元素型態，如GL_BYTE、GL_FLOAT等
|normalized|是否要將以整數格式儲存的值標準化到[0, 1]或[-1, 1]之間，填入GL_TRUE或GL_FALSE
|stride|兩個資料間的間隔，0為無間隔
|pointer|第一個屬性資料的起始位置在目前VBO中的偏移量

<font color=#FF0000>**注意**</font>：pointer參數需為c_types格式中的c_void_p，單位為bit。

3. **影像渲染**
使用`glDrawArrays()`函式將前述設定之頂點資料與顏色之料進行渲染，並將其部署於視窗顯示函式中，詳細說明可參考[2022/04/29 單機實現AR演算法（3）](https://hackmd.io/cI9JoMX9S4epY58TWmEB-w?view#20220429-%E5%96%AE%E6%A9%9F%E5%AF%A6%E7%8F%BEAR%E6%BC%94%E7%AE%97%E6%B3%95%EF%BC%883%EF%BC%89)。程式如下所示。程式中的相關參數意義如下表所示。

```python=
def My_Display():
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT) 
    glDrawArrays(mode, first, count)
    glutSwapBuffers()
```

|  參數名稱|參數意義|
|:---:|:---
|mode|陣列繪圖模式，繪圖模式可參考《OpenGL 3D繪圖互動程式設計》p6-11 表06-006
|first|指定開始的頂點陣列
|count|陣列大小

## 程式碼整理
### OpenGL相機設定
```python=
#!/usr/bin/env python
# coding: utf-8

# # AR演算法3_測試成功
# 將OpenGL與OpenCV結合，使用OpenCV獲得現場影像與相機參數，與OpenGL選染空間結合，創造出AR影像渲染效果。

import cv2
import numpy as np

from OpenGL.GLUT import *
from OpenGL.GLU import *
from OpenGL.GL import *

# ## matplotlib
# 使用matplotlib代替OpenCV顯示影像，讓影像顯示在Jupyter Notebook裡面。使用`conda install -c conda-forge matplotlib`指令安裝，本次安裝3.5.1版

from matplotlib import pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')

def imshow(img):
    img2 = img[:, :, ::-1] #BGR2RGB
    plt.imshow(img2)
    plt.axis('off') #關閉坐標軸
    
    plt.show()

# ## 導入測試資料
# 1. 導入預先創建好的相機參數
# 2. 導入測試用照片

parameter = np.load('test_Camera_parameter.npz')
cameraMatrix = parameter['mtx']
distCoeffs = parameter['dist']

img = cv2.imread('test_img.png')
imshow(img)


# ## AR演算法實現

dictionary = cv2.aruco.Dictionary_get(10)
parameters = cv2.aruco.DetectorParameters_create()


# ### 相機影像處理
# 實際使用時用影像串流，目前使用照片進行測試。

# 轉化為灰度圖像
gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
# 檢測圖像中標記
corners, ids, rejected_corners = cv2.aruco.detectMarkers(gray_img, dictionary, parameters=parameters)
# 繪制檢測標記
marker_img = cv2.aruco.drawDetectedMarkers(image=img, corners=corners, ids=ids, borderColor=(0, 255, 0))

if ids is not None:
    # rvecs, tvecs分別是角點中每個標記的旋轉和平移向量
    rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, 1, cameraMatrix, distCoeffs)
    
    # 繪制系統軸
    for rvec, tvec in zip(rvecs, tvecs):
        cv2.aruco.drawAxis(marker_img, cameraMatrix, distCoeffs, rvec, tvec, 1)
        
imshow(img)

# ### 矩陣生成測試01
# 生成model_view與projection矩陣    
# [參考](https://github.com/BryceQing/OPENCV_AR)

def extrinsic2ModelView(RVEC, TVEC, R_vector = True):
    """[Get modelview matrix from RVEC and TVEC]
    Arguments:
        RVEC {[vector]} -- [Rotation vector]
        TVEC {[vector]} -- [Translation vector]
    """

    R, _ = cv2.Rodrigues(RVEC)

    Rx = np.array([
        [1, 0, 0],
        [0, -1, 0],
        [0, 0, -1]
    ])

    TVEC = TVEC.flatten().reshape((3, 1))

    #transform_matrix = Rx @ np.hstack((R, TVEC))
    transform_matrix = np.dot(Rx, np.hstack((R, TVEC)))
    M = np.eye(4)
    
    M[:3, :] = transform_matrix
    return M.T.flatten()

def intrinsic2Project(MTX, width, height, near_plane=0.01, far_plane=100.0):
    """[Get ]
    Arguments:
        MTX {[np.array]} -- [The camera instrinsic matrix that you get from calibrating your chessboard]
        width {[float]} -- [width of viewport]]
        height {[float]} -- [height of viewport]
    Keyword Arguments:
        near_plane {float} -- [near_plane] (default: {0.01})
        far_plane {float} -- [far plane] (default: {100.0})
    Returns:
        [np.array] -- [1 dim array of project matrix]
    """
    P = np.zeros(shape=(4, 4), dtype=np.float32)

    fx, fy = MTX[0, 0], MTX[1, 1]
    cx, cy = MTX[0, 2], MTX[1, 2]

    P[0, 0] = 2 * fx / width
    P[1, 1] = 2 * fy / height
    P[2, 0] = 1 - 2 * cx / width
    P[2, 1] = 2 * cy / height - 1
    P[2, 2] = -(far_plane + near_plane) / (far_plane - near_plane)
    P[2, 3] = -1.0
    P[3, 2] = - (2 * far_plane * near_plane) / (far_plane - near_plane)

    return P.flatten()

heigh = img.shape[0]
width = img.shape[1]

#modelView = extrinsic2ModelView(rvecs[0, :, :], tvecs[0, :, :])
modelView = extrinsic2ModelView(rvec, tvec)
projection = intrinsic2Project(cameraMatrix, width, heigh)

# ### OpenGL渲染測試

def display():
    global modelView, projection
    
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
    glClearColor(255, 255, 255, 0)
    
    glMatrixMode(GL_PROJECTION)
    glLoadIdentity()
    glMultMatrixf(projection)

    glMatrixMode(GL_MODELVIEW)
    glLoadIdentity()
    glLoadMatrixf(modelView)
    
    glColor3b(0, 125, 0)
    glutWireTeapot(0.5)
    
    glutSwapBuffers()

glutInit()
glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH)
glutInitWindowPosition(100, 100)
glutInitWindowSize(640, 480)
glutCreateWindow(b"OpenGL")
glutDisplayFunc(display)
glutIdleFunc(display)
glClearColor(0, 0, 0, 0)
glutMainLoop()
```
### 影像渲染器（Shader）與緩衝器（Buffer）建置

```python=
from OpenGL.GLUT import *
from OpenGL.GLU import *
from OpenGL.GL import *
import numpy as np
from ctypes import * #glVertexAttribPointer 最後一個數值需用ctypes格式，不然會跑不動
import sys

#頂點渲染器程式（vs）
VERTEX_SHADER = """   
#version 410
 
layout(location = 0) in vec3 iv3vertex;
layout(location = 1) in vec3 iv3color;
 
out vec3 vv3color;

void main()
{
	gl_Position = vec4(iv3vertex, 1.0);
    vv3color = iv3color;
}
"""
 
#片段渲染器程式（fs）
FRAGMENT_SHADER = """ 
#version 410

in vec3 vv3color;

layout(location = 0) out vec4 fragColor;

void main()
{
    fragColor = vec4(vv3color, 1.0);
}
"""

def My_Init():
    glClearColor(1.0, 1.0, 1.0, 1.0)
    glEnable(GL_DEPTH_TEST)
    glDepthFunc(GL_LEQUAL)
    
    # Initialize shaders
    ########################
    program = glCreateProgram()
     
    vs = glCreateShader(GL_VERTEX_SHADER)
    fs = glCreateShader(GL_FRAGMENT_SHADER)
    
    glShaderSource(vs, VERTEX_SHADER)
    glShaderSource(fs, FRAGMENT_SHADER)
    
    glCompileShader(vs)
    glCompileShader(fs)
    
    glAttachShader(program, vs)
    glAttachShader(program, fs)
    
    glLinkProgram(program)
     
    glUseProgram(program)
    ########################
    
    data = np.array([[-0.5, -0.4, 0.0],
                     [0.5, -0.4, 0.0],
                     [0.0, 0.6, 0.0],
                     [1.0, 0.0, 0.0],
                     [0.0, 1.0, 0.0],
                     [0.0, 0.0, 1.0]], dtype = "float32")
    
    data = data.flatten()
    
    # Create buffer
    ########################
    buffer = glGenBuffers(1)
    
    glBindBuffer(GL_ARRAY_BUFFER, buffer)
    
    glBufferData(GL_ARRAY_BUFFER, data.nbytes, data, GL_STATIC_DRAW)
    
    vao = glGenVertexArrays(1)
    glBindVertexArray(vao)
    
    glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 0, None)
    glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 0, c_void_p(36)) #位移一個元素4bit，共為一9個元素36bit，型別要用ctypes的c_void_p
    
    glEnableVertexAttribArray(0)
    glEnableVertexAttribArray(1)

def My_Display():
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)     
    glDrawArrays(GL_TRIANGLES, 0, 3)
    glutSwapBuffers()
    
def main():
    glutInit()
    glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE | GLUT_DEPTH)
    
    glutInitWindowPosition(100, 100)
    glutInitWindowSize(640, 480)
    glutCreateWindow(b"Framework")
    
    My_Init()
    glutDisplayFunc(My_Display)
    
    glutMainLoop()
    
if __name__ == "__main__":
    main()
```

## Reference
### 相機矩陣轉換

[[AR]實作-openCV&openGL](https://home.gamer.com.tw/creationDetail.php?sn=4093935)
這篇主要講述OpenCV與OpenGL建構AR應用程式，**以C++進行編程**

[Converting OpenCV cameras to OpenGL cameras.](https://amytabb.com/tips/tutorials/2019/06/28/OpenCV-to-OpenGL-tutorial-essentials/)
講述如何將OpenCV相機參數轉換至OpenGL相機中

[Code for OpenCV cameras to OpenGL cameras.](https://amytabb.com/ts/2019_07_02/)

[Dissecting the Camera Matrix, Part 2: The Extrinsic Matrix](http://ksimek.github.io/2012/08/22/extrinsic/)
講解相機外部矩陣在OpenCV與OpenGL間之轉換

[Dissecting the Camera Matrix, Part 3: The Intrinsic Matrix](http://ksimek.github.io/2013/08/13/intrinsic/)
講解相機內部矩陣在OpenCV與OpenGL間之轉換

[Programming Computer Vision with Python by Jan Erik Solem - Chapter 4. Camera Models and Augmented Reality](https://www.oreilly.com/library/view/programming-computer-vision/9781449341916/ch04.html)
這是一本使用python程式語言為範例的電腦視覺程式參考書

[BryceQing/OPENCV_AR](https://github.com/BryceQing/OPENCV_AR)
這個Github儲存庫提供了一個OpenCV與OpenGL程式結合的AR範例（包含相機座標轉換）

### 渲染器與緩衝器建置
[OpenGL 3D 繪圖互動程式設計](https://www.flag.com.tw/books/product/FT755)，p4-1～p5-13
這是一本以C++程式語言為主的OpenGL程式工具書

[（Python OpenGL）【3】着色器 PyOpenGL](https://www.cnblogs.com/WSX1994/p/9096385.html)
這裡提供了一個建置渲染器的範例

[PyOpenGL之glVertexAttribPointer](https://blog.csdn.net/sunjinshengli/article/details/106976051)
解釋`glVertexAttribPointer()`參數設置問題

[OpenGL 3/4 glVertexAttribPointer stride and offset miscalculation](https://stackoverflow.com/questions/16380005/opengl-3-4-glvertexattribpointer-stride-and-offset-miscalculation)
解釋`glVertexAttribPointer()`中的pointer參數如何計算

[How to specify buffer offset with PyOpenGL](https://stackoverflow.com/questions/11132716/how-to-specify-buffer-offset-with-pyopengl)
解釋如何使用 PyOpenGL 指定緩衝區偏移量

[Python的學習（三十二）---- ctypes庫的使用整理](https://www.796t.com/content/1546708889.html)
Python中c_types函式庫使用教學

[细说Python中numpy的nbytes方法](https://blog.csdn.net/weixin_44915226/article/details/104271027)
物件大小計算方法之間的結果差異